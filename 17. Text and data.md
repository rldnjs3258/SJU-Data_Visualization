# 1. Text와 시각화
## (1) Text란?
  - Text는 문자열로 구성 되어 있다.
  - Text 데이터의 분석 및 시각화는 SNS를 분석할 때 많이 이용한다.
  - ex) 책, 소설, 블로그, 웹 페이지, 이메일, 태그, 댓글, 프로그래밍 언어, 로그 등

<br>

## (2) Text와 시각화 방법
  - Text를 직접 시각화 한다. ex) Word Cloud
  - Text를 모델을 이용해 Text의 단어 수, 단어 순서 등을 분석해 시각화 한다.

<br>
<hr>
<br>

# 2. Text 시각화의 필요성
 - Understanding : 문서의 키포인트를 알기 위해서
 - Grouping : 문서의 키워드를 그룹핑 하기 위해서
 - Comparison : 문서의 키워드를 서로 비교하기 위해서
 - Correlation : 문서의 키워드와 패턴을 분석하기 위해서


<br>
<hr>
<br>


# 3. Text 시각화 평가
##### (1) Text 시각화를 독자가 해석할 수 있는가?
##### (2) Text 시각화가 모델의 속성을 잘 전달하는가?
##### (3) Text 시각화의 모델을 신뢰할 수 있는가?
##### (4) 모델이 Text를 분석하는 방법이 논리적인가?


<br>
<hr>
<br>


# 4. Text 시각화의 Challenges
## (1) 단어와 고차원
  - Text는 단어마다 차원이 존재한다. 따라서 어떻게 고차원을 분석하고 시각화 할지 고려해야 한다.

<br>

## (2) Context & Semantics
  - Text를 시각화 할 때, 단어의 문맥을 잘 전달할 수 있는 방법을 고려해야 한다.

<br>

## (3) Modeling Abstraction
  - 문서에 포함 된 많은 단어들을 어떻게 추상화 할 지 고려해야 한다.


<br>
<hr>
<br>


# 5. Text의 Nominal type
##### (1) 연관된 단어 : ('Hong' 'Kong'), ('San' 'Francisco'), ('Bay' 'Area')
##### (2) 순서가 있는 단어 : April, February, January, June, March, May
##### (3) Membership : Tennis, January, Swimming, Hiking
##### (4) Hierarchy, antonyms & synonyms, entities, ...


<br>
<hr>
<br>


# 6. Text Processing Pipeline
## 1) Tokenization
  - 문서의 문장들을 Token으로 쪼갠다.
  - Nltk 혹은 Spacy 라이브러리를 이용한다.
  - Stop words를 삭제한다.

<br>

## 2) Stemming
  - 과거, 현재, 미래 등을 하나의 시점으로 정제한다.
  - Lemmatization

<br>

## 3) Ordered list of terms


<br>
<hr>
<br>


# 7. Tokeniation과 Stemming시 주의사항
 - Stemming시 'txt', 'u', 'l8r!' 등을 사전을 이용해 정제할 수 있어야 한다.
 - 스펠링이 틀렸거나, 오타가 있을 경우 사전을 이용해 정제할 수 있어야 한다.


<br>
<hr>
<br>


# 8. Bag of Words Model
 - 문서 안의 단어 순서 관계를 무시하고 단어의 빈도수를 카운트 한다.
 - Document-term 행렬을 통해 aggregate 한다.
 - Bag of Words Model을 통해 계산 된 값은 단어와 문서의 관련성을 나타낸다.


<br>
<hr>
<br>


# 9. Tag Clouds
## (1) Tag Clouds란?
  - 문서의 키포인트를 클라우드 모양의 시각화로 제공한다.

<br>

## (2) 한계점
  - 시각화시 사이즈와 위치를 Optimal하게 만들기 쉽지 않다.
  - 시각화시 독자가 단어를 잘못 해석할 수 있다.
  - 문서에서 등장 빈도수가 많다고 문서를 대표하는 키워드는 아니다.
  - 시각화시 문맥을 반영하지 못한다.


<br>
<hr>
<br>


# 10. Keywords Weighting
## (1) TF (Term Frequency)
  - tf(td) : count(t) in d
  - log frequency : log(1 + tf(td))
  - 비율 표시를 위한 정규화 : tf(td) / ∑(t)tf(td)

<br>

## (2) TF-IDF (Term Frequency by Inverse Document Frequency)
  - tf.idftd = log(1 + tftd) X log(N/dft)
  - dft = # docs containing t;
  - N = # of docs

<br>

## (3) G^2 (Probability of different word frequency)
  - E1 = |d| x (tftd + tft(C-d)) / |C|
  - E2 = |C-d| x (tftd + tft(C-d)) / |C|
  - G2 = 2 x (tftd log(tftd/E1) + tft(C-d) log(tft(C-d)/E2))



<br>
<hr>
<br>

# 11. Frequency Statistics의 한계점
## (1) Frequency Statistics의 한계점
  - Frequency Statistics는 일반적으로 unigrams를 이용한다.
  - 빈도수를 계산하여 best description을 찾기 쉽지 않다.
  - 그럼에도 불구하고 Frequency Statistics는 많이 쓰인다.

<br>

## (2) Bag of Words의 한계점
  - Bag of Words는 Grammar/part-of-speech, 문서 내 위치 및 순서, entities와 같은 정보를 무시한다.
  - 쉬운 계산 방법이다.


<br>
<hr>
<br>


# 12. Term Commonness
 - log(tf(w)) / log(tf(the))
 - Term Commonness를 통해 해당 문서가 '사람들이 이해하기 쉬운 문서'인지 확인할 수 있다.
 - 사람들이 많이 이용하는 단어인지, 논문에 많이 나오는 단어인지를 확인한다.


<br>
<hr>
<br>


# 13. 정보 검색 (Information Retrieval)
 - 인터넷에서 정보를 검색 할 때, 확률 분포를 통해 검색한 단어와 문서가 얼마나 비슷한지 계산된다.
 - 쿼리 문자열과 문서의 매칭도가 계산된다.
 - 정보 검색 결과를 상황에 맞게 시각화 하는 것이 좋다.
